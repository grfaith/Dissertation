{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYFPuVWNfY6d52ND72upiJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grfaith/Dissertation/blob/master/Finding_explore_kw_hits_in_AS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY00mYewtFuU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "###Installs\n",
        "\n",
        "!pip install datasets\n",
        "!pip install ipympl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is code for looking at a range of years in the American Stories data and finding articles which appear with the string 'explor' and saving their information to disk."
      ],
      "metadata": {
        "id": "ynmUlyRjtbwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import tqdm as tq\n",
        "from google.colab import files\n",
        "\n",
        "start_year = 1820\n",
        "end_year = 1827 #This is end-year exclusive (I think)"
      ],
      "metadata": {
        "id": "Fy1ZCAu_twkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell for loading files from local drive\n",
        "kw_file = files.upload()\n",
        "\n",
        "# Specify custom column names\n",
        "column_names = [\"kword\", \"kwyear\"]\n",
        "\n",
        "# Read the uploaded CSV file into a DataFrame with custom column names\n",
        "for fn in kw_file.keys():\n",
        "    kw_df = pd.read_csv(fn, names=column_names, header=None)\n",
        "\n",
        "# Display information about the uploaded file\n",
        "for fn in kw_file.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "          name=fn, length=len(kw_file[fn])))\n",
        "\n"
      ],
      "metadata": {
        "id": "IudtCh2We58L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining function to load dataset\n",
        "\n",
        "def load_text_dataset(dataset_year_str):\n",
        "    \"\"\"\n",
        "    This function pulls a dataset of a specific year from the HuggingFace Hub.\n",
        "\n",
        "    Parameters:\n",
        "        dataset_year (int): The year of the dataset to be pulled..\n",
        "\n",
        "    Returns:\n",
        "        dataset_article_level: dataset for appropriate year\n",
        "    \"\"\"\n",
        "    # Download data for the dataset year at the associated article level (Default)\n",
        "    # dataset = load_dataset(\"dell-research-harvard/AmericanStories\", \"subset_years\", year_list=[dataset_year])\n",
        "\n",
        "    # now let's load our data, we have to specify the huggingface location of our\n",
        "    # data, the fact that we want to have a subset of years, and our desired years\n",
        "    dataset_article_level=load_dataset(\"dell-research-harvard/AmericanStories\",\n",
        "                                      \"subset_years\",\n",
        "                                       year_list=[dataset_year_str],\n",
        "                                       trust_remote_code=True\n",
        "                                       )\n",
        "\n",
        "    return dataset_article_level"
      ],
      "metadata": {
        "id": "Ek6QpIYKyGpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Function to filter kw data in use based on years of discovery in kw file.\n",
        "\n",
        "def get_kw(dataset_year_str):\n",
        "    \"\"\"\n",
        "    This function loads a CSV file to create a DataFrame and filters out keywords where the second column is less than 1774\n",
        "    Parameters:\n",
        "        kw_file loaded from prompt above\n",
        "    Returns:\n",
        "        pandas.DataFrame: The filtered Data\n",
        "    \"\"\"\n",
        "    # Convert dataset_year to integer\n",
        "    dataset_year = int(dataset_year_str)\n",
        "\n",
        "    # Filter the rows based on the condition\n",
        "    kw_df_filter = kw_df[kw_df['kwyear'] <= dataset_year]\n",
        "\n",
        "    # print(kw_df_filter)\n",
        "\n",
        "    return kw_df_filter\n",
        "\n"
      ],
      "metadata": {
        "id": "-SvYzj3t89IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_kw(dataset_year_str, kw_df_filter, dataset_article_level):\n",
        "    \"\"\"\n",
        "    This function processes words in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        kw_df_filter (pandas.DataFrame): The DataFrame containing keywords\n",
        "        dataset_article (DatasetDict): A dictionary-like object containing datasets for different years.\n",
        "\n",
        "    Returns:\n",
        "        dataset_year_hits (DataFrame or None): A DataFrame containing results if found, or None if no results were found.\n",
        "    \"\"\"\n",
        "    # Creating an empty dataframe\n",
        "    current_year_df = pd.DataFrame()\n",
        "\n",
        "    for index, row in kw_df_filter.iterrows():\n",
        "        explore_kw = row.iloc[0]\n",
        "        result_df = kw_pair_search(dataset_article_level, dataset_year_str, explore_kw)\n",
        "        # Concatenate the single search result onto the results DataFrame\n",
        "        current_year_df = pd.concat([current_year_df, result_df], ignore_index=True)\n",
        "\n",
        "    return current_year_df\n"
      ],
      "metadata": {
        "id": "Mf7CjY-AznRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kw_pair_search(dataset_article, dataset_year, explor_kw):\n",
        "    \"\"\"\n",
        "    This function searches through the dataset, by kw, to look for matching articles.\n",
        "\n",
        "    Parameters:\n",
        "        dataset_article (DatasetDict): A dictionary-like object containing datasets for different years.\n",
        "        dataset_year (int): The year of the dataset.\n",
        "        explor_kw (str): The keyword to search for.\n",
        "\n",
        "    Returns:\n",
        "        df_of_articles_containing_two_words (DataFrame): A DataFrame containing article IDs and article texts of articles containing both keywords.\n",
        "    \"\"\"\n",
        "    # Access the dataset for the specific year\n",
        "    year_dataset = dataset_article[dataset_year]\n",
        "\n",
        "    # Access the 'raw_data_string' column\n",
        "    articles = year_dataset['article']\n",
        "\n",
        "    # Create empty list to store matching articles\n",
        "    articles_containing_two_words = []\n",
        "\n",
        "    for article_n, article_text in enumerate(articles):\n",
        "        article_text = article_text.lower()\n",
        "        if \"explor\" in article_text and explor_kw in article_text:\n",
        "            # Append the matching article information to the list\n",
        "            articles_containing_two_words.append({\n",
        "                'article_year': dataset_year,\n",
        "                'keyword hit': explor_kw,\n",
        "                'row_number': article_n,\n",
        "                'article_ID': dataset_article[dataset_year][article_n][\"article_id\"],\n",
        "                'newspaper_name': dataset_article[dataset_year][article_n][\"newspaper_name\"],\n",
        "                'edition': dataset_article[dataset_year][article_n][\"edition\"],\n",
        "                'date': dataset_article[dataset_year][article_n][\"date\"],\n",
        "                'page': dataset_article[dataset_year][article_n][\"page\"],\n",
        "                'headline': dataset_article[dataset_year][article_n][\"headline\"],\n",
        "                'byline': dataset_article[dataset_year][article_n][\"byline\"],\n",
        "                # 'article': dataset_article[dataset_year][article_n][\"article_id\"],\n",
        "            })\n",
        "\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    df_of_articles_containing_two_words = pd.DataFrame(articles_containing_two_words)\n",
        "    # if not df_of_articles_containing_two_words.empty:\n",
        "    #  print(df_of_articles_containing_two_words)\n",
        "    # input(\"Press Enter to continue...\")\n",
        "    return df_of_articles_containing_two_words"
      ],
      "metadata": {
        "id": "mVewWmA8BqHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame()\n",
        "\n",
        "for loop_year in range(start_year, end__year):\n",
        "    dataset_year_str = str(loop_year)\n",
        "    try:\n",
        "        # Load dataset for current loop_year\n",
        "        dataset_article_level = load_text_dataset(dataset_year_str)\n",
        "\n",
        "        # Get valid keyword filters for current year\n",
        "        kw_df_filter = get_kw(loop_year)\n",
        "        # print (\"Processing year\", dataset_year_str)\n",
        "\n",
        "        # Process keyword filters and dataset for current loop year to get hits\n",
        "        year_search_result = process_kw(dataset_year_str,kw_df_filter, dataset_article_level)\n",
        "\n",
        "        # if not year_search_result.empty:\n",
        "        #    print(year_search_result)\n",
        "\n",
        "\n",
        "        # Concatenate the single search result onto the results DataFrame\n",
        "        results = pd.concat([results, year_search_result])\n",
        "\n",
        "    except ValueError:\n",
        "        print(f\"Dataset empty for {dataset_year_str}. Moving to the next year.\")\n",
        "        continue\n",
        "\n",
        "results = results.reset_index(drop=True)\n",
        "results.to_csv('AS_Explor_KW_Hits_Raw.csv', index=False)\n",
        "files.download(\"AS_Explor_KW_Hits_Raw.csv\")\n",
        "print(\"Finished\")\n"
      ],
      "metadata": {
        "id": "aUp2-Y38I1S7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = results.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "kW85hKicF3Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (results)\n"
      ],
      "metadata": {
        "id": "4sq5PxGbcTV7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foo=load_text_dataset(\"1827\")"
      ],
      "metadata": {
        "id": "Qv98VoTrdq28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"AS_Explor_KW_Hits_Raw.csv\")"
      ],
      "metadata": {
        "id": "KHL5hKbBR8cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pg_JqwMNIsaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***SOLVENT FRONT\n",
        "\n",
        "Code above this line has been rechecked against AmStories and works properly.***"
      ],
      "metadata": {
        "id": "njhvRm39ElEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Functions\n",
        "\n",
        "# Def Load Text Dataset\n",
        "# Def Process = Search on two keywords\n",
        "# AmStory light cleanup\n",
        "# GPT OCR Cleanup\n",
        "# GPT Explore space\n",
        "# Evaluate explore space (for multiple runs)\n",
        "# Sort by vote counts into six pops (5Y, 0N - 4/1, 3/2, 2/3, 1/4, 0/5)"
      ],
      "metadata": {
        "id": "LoC3vAt4tzEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Functions\n",
        "\n",
        "*1.  Def Load Dataset*\n",
        "\n",
        "*2.  Def Process = Search on two keywords*\n",
        "3.  GPT OCR Cleanup\n",
        "4.  GPT Explore space\n",
        "5.  Evaluate explore space (for multiple runs)\n",
        "6.  Sort by vote counts into six pops (5Y, 0N - 4/1, 3/2, 2/3, 1/4, 0/5)\n",
        "7.  Match with coded results\n",
        "8.  Validate"
      ],
      "metadata": {
        "id": "uwuKrrPExW3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an empty dataframe based on the features of the dataset\n",
        "\n",
        "# Assuming dataset_article_level[\"1862\"].features is a dictionary\n",
        "features_dict = dataset_article_level[dataset_year_str].features\n",
        "\n",
        "# Create a DataFrame from the dictionary\n",
        "current_year_df = pd.DataFrame([features_dict])\n"
      ],
      "metadata": {
        "id": "fXbM3cbPFtSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loop\n",
        "\n",
        "  #For each year in range\n",
        "     #Load dataset, and then process\n",
        "        #Process loop on keywords\n",
        "        #Save to file\n",
        "\n",
        "#Save to file (checkpoint for raw explor corpus)\n",
        "\n",
        "# Mechanics for next pending size\n",
        "# Run light AmStory cleanup\n",
        "# Run GPT OCR cleanup\n",
        "# Run 5x explore space decision via GPT\n",
        "# Split out results based on similar votes\n",
        "# Add in links to existing coding responses\n",
        "# Validate decisions (maybe sample of yes/no and some larger portion of mixed votes to get a feel for data)\n",
        "\n",
        "# Analysis?  Decide next steps?  Consider Hathi Dictionary run?\n"
      ],
      "metadata": {
        "id": "ccFuAdVKt1xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame to inspect the data\n",
        "print(\"First few rows of the DataFrame:\")\n",
        "print(results.head())\n",
        "\n",
        "# Get the shape of the DataFrame\n",
        "print(\"\\nShape of the DataFrame:\")\n",
        "print(results.shape)\n",
        "\n",
        "# Get the data types of each column in the DataFrame\n",
        "print(\"\\nData types of each column:\")\n",
        "print(results.dtypes)\n",
        "\n",
        "# Optionally, you can get a concise summary of the DataFrame\n",
        "print(\"\\nSummary of the DataFrame:\")\n",
        "print(results.info())"
      ],
      "metadata": {
        "id": "ac_5TSyK9GNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}